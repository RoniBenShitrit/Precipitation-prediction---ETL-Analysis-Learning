{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Learning\n",
    "In this part we will try to train a machine learning model to try and predict the amount of Precipitation that will be recorded\n",
    "by a given <b>Station</b> at a specific <b>Year</b> and <b>Month</b>.<br>\n",
    "As we have seen in our analysis, there doesn't seem to be such a strong correlation between Precipitation and Geospatial information such as longitude and Elevation.\n",
    "We have also seen that there isn't any correlation (both linear and Nonlinear) with any of the other\n",
    "variables TMIN, TMAX, and SNWD, so we will avoid using models like Linear Regression or Generalized Linear Regression. <br>\n",
    "Therefore, we will attempt to use a Random Forest Regressor and a Decision Tree Regressor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by importing all the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\n",
    "    'PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4,\"\\\n",
    "                             \"com.microsoft.azure:spark-mssql-connector:1.0.2 pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start our spark session and define out global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_spark(app_name):\n",
    "    spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc\n",
    "\n",
    "\n",
    "server_name = \"ServerName\"\n",
    "database_name = \"DatabaseName\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "table_name = \"TableName\"\n",
    "username = \"UserName\"\n",
    "password = \"Password\"\n",
    "spark, sc = init_spark('project')\n",
    "kafka_server = 'dds2020s-kafka.eastus.cloudapp.azure.com:9092'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load our data from the data store into two different Pyspark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jdbcDF = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", table_name)\\\n",
    "    .option(\"user\", username)\\\n",
    "    .option(\"password\", password).load()\n",
    "\n",
    "stationData = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", 'StationData')\\\n",
    "    .option(\"user\", username)\\\n",
    "    .option(\"password\", password).load()\n",
    "jdbcDF.createOrReplaceTempView(\"DataStore_Stage2\")\n",
    "stationData.createOrReplaceTempView(\"stationData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data store, we will build our dataset.<br>\n",
    "Our chosen features for the predictive model are:<br>\n",
    "<ul>\n",
    "<li> <b>TMAX & TMIN</b> from the same Month Year and Station of which we are trying to predict it's PRCP levels.</li>\n",
    "<li> The <b>Season</b> of the given month.</li>\n",
    "<li> <b> Elevation & Latitude </b> of the station.</li>\n",
    "<li> The average <b>SNWD</b> two seasons prior to the current season.</li>\n",
    "</ul><br>\n",
    "Our hypothesis is that given a station, year and month the model will be able to capture the phenomena of melting ice as a source of precipitation.<br>\n",
    "We hope to see that given a Season with high temperatures (both maximum and minimum) with high elevation and low latitude and a high amount of snow two seasons ago will produce high precipitation,\n",
    "while low tempretures with low elevation and high latitudes and low snow amounts will produce lower amounts of precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jdbcDF = spark.sql(\n",
    "    \"\"\"select d1.StationId ,d1.Year_Rec,d1.Month_Rec, d1.Season,d1.PRCP,d1.TMAX,d1.TMIN,s1.Elevation,\n",
    "        s1.Latitude, avg(d2.SNWD) Season_SNWD\n",
    "        from DataStore_Stage2 d1, DataStore_Stage2 d2, StationData s1\n",
    "        where d1.StationId = d2.StationId and\n",
    "              d1.StationId = s1.StationId and\n",
    "              ((d1.Season % 4 = (d2.Season % 4 ) + 2 and d1.Year_Rec=d2.Year_Rec and d1.Season in (3,4)) or\n",
    "              (d1.Season % 4 = (d2.Season % 4 ) + 2 and d1.Year_Rec=d2.Year_Rec+1 and d1.Season in (1,2)))\n",
    "            and d2.SNWD is not null and d1.PRCP is not null and d1.TMIN is not null and d1.TMAX is not null\n",
    "        group by d1.StationId, d1.Year_Rec, d1.Month_Rec, d1.Season, d1.PRCP, d1.TMAX, d1.TMIN,\n",
    "        s1.Elevation,s1.Latitude\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['Season', 'TMAX', 'TMIN', 'Elevation','Latitude', 'Season_SNWD'],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(jdbcDF).drop(\"StationId\", \"Year_Rec\", \"Month_Rec\", \"TMAX\", \"TMIN\", \"SNWD\", 'Season_SNWD',\\\n",
    "                                          \"Elevation\", 'Latitude', 'Season')\\\n",
    "    .withColumn(\"label\", F.col(\"PRCP\")).drop(\"PRCP\")\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data in a ratio of 7:3 so that we could try to have balanced samples in both the train and test set, all the while preserving a ratio that is translated to a large train set and a smaller test set. <br>\n",
    "We will train a few models and compare results. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = output.randomSplit([0.7, 0.3])\n",
    "rf1 = RandomForestRegressor(featuresCol=\"indexedFeatures\", maxDepth=5, numTrees=10)\n",
    "rf2 = RandomForestRegressor(featuresCol=\"indexedFeatures\", maxDepth=5, numTrees=20)\n",
    "rf3 = RandomForestRegressor(featuresCol=\"indexedFeatures\", maxDepth=10, numTrees=10)\n",
    "rf4 = RandomForestRegressor(featuresCol=\"indexedFeatures\", maxDepth=10, numTrees=20)\n",
    "dt1 = DecisionTreeRegressor(featuresCol=\"indexedFeatures\", maxDepth=10)\n",
    "dt2 = DecisionTreeRegressor(featuresCol=\"indexedFeatures\", maxDepth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[featureIndexer, rf1])\n",
    "pipeline2 = Pipeline(stages=[featureIndexer, rf2])\n",
    "pipeline3 = Pipeline(stages=[featureIndexer, rf3])\n",
    "pipeline4 = Pipeline(stages=[featureIndexer, rf4])\n",
    "pipeline5 = Pipeline(stages=[featureIndexer, dt1])\n",
    "pipeline6 = Pipeline(stages=[featureIndexer, dt2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "model2 = pipeline2.fit(trainingData)\n",
    "model3 = pipeline3.fit(trainingData)\n",
    "model4 = pipeline4.fit(trainingData)\n",
    "model5 = pipeline5.fit(trainingData)\n",
    "model6 = pipeline6.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting our models we will begin predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions_rf1 = model.transform(testData)\n",
    "train_predictions_rf1 = model.transform(trainingData)\n",
    "\n",
    "test_predictions_rf2 = model2.transform(testData)\n",
    "train_predictions_rf2 = model2.transform(trainingData)\n",
    "\n",
    "test_predictions_rf3 = model3.transform(testData)\n",
    "train_predictions_rf3 = model4.transform(trainingData)\n",
    "\n",
    "test_predictions_rf4 = model4.transform(testData)\n",
    "train_predictions_rf4 = model5.transform(trainingData)\n",
    "\n",
    "test_predictions_dt1 = model5.transform(testData)\n",
    "train_predictions_dt1 = model5.transform(trainingData)\n",
    "\n",
    "test_predictions_dt2 = model6.transform(testData)\n",
    "train_predictions_dt2 = model6.transform(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MaxDepth=5, NumTrees=10 - RMSE on test data = 12.1776\n",
      "Random Forest MaxDepth=5, NumTrees=10 - RMSE on train data = 12.1992\n",
      "Random Forest MaxDepth=5, NumTrees=20 - RMSE on test data = 12.11\n",
      "Random Forest MaxDepth=5, NumTrees=20 - RMSE on train data = 12.136\n",
      "Random Forest MaxDepth=10, NumTrees=10 - RMSE on test data = 11.3922\n",
      "Random Forest MaxDepth=10, NumTrees=10 - RMSE on train data = 11.2317\n",
      "Random Forest MaxDepth=10, NumTrees=20 - RMSE on test data = 11.3508\n",
      "Random Forest MaxDepth=10, NumTrees=20 - RMSE on train data = 11.1468\n",
      "Decision Tree MaxDepth=10 - RMSE on test data = 11.4237\n",
      "Decision Tree MaxDepth=10 - RMSE on train data = 11.1468\n",
      "Decision Tree MaxDepth=5 - RMSE on test data = 12.1716\n",
      "Decision Tree MaxDepth=5 - RMSE on train data = 12.2021\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_test_rf1 = evaluator.evaluate(test_predictions_rf1)\n",
    "rmse_train_rf1 = evaluator.evaluate(train_predictions_rf1)\n",
    "\n",
    "print(\"Random Forest MaxDepth=5, NumTrees=10 - RMSE on test data = %g\" % rmse_test_rf1)\n",
    "print(\"Random Forest MaxDepth=5, NumTrees=10 - RMSE on train data = %g\" % rmse_train_rf1)\n",
    "\n",
    "rmse_test_rf2 = evaluator.evaluate(test_predictions_rf2)\n",
    "rmse_train_rf2 = evaluator.evaluate(train_predictions_rf2)\n",
    "\n",
    "print(\"Random Forest MaxDepth=5, NumTrees=20 - RMSE on test data = %g\" % rmse_test_rf2)\n",
    "print(\"Random Forest MaxDepth=5, NumTrees=20 - RMSE on train data = %g\" % rmse_train_rf2)\n",
    "\n",
    "rmse_test_rf3 = evaluator.evaluate(test_predictions_rf3)\n",
    "rmse_train_rf3 = evaluator.evaluate(train_predictions_rf3)\n",
    "\n",
    "print(\"Random Forest MaxDepth=10, NumTrees=10 - RMSE on test data = %g\" % rmse_test_rf3)\n",
    "print(\"Random Forest MaxDepth=10, NumTrees=10 - RMSE on train data = %g\" % rmse_train_rf3)\n",
    "\n",
    "rmse_test_rf4 = evaluator.evaluate(test_predictions_rf4)\n",
    "rmse_train_rf4 = evaluator.evaluate(train_predictions_rf4)\n",
    "\n",
    "print(\"Random Forest MaxDepth=10, NumTrees=20 - RMSE on test data = %g\" % rmse_test_rf4)\n",
    "print(\"Random Forest MaxDepth=10, NumTrees=20 - RMSE on train data = %g\" % rmse_train_rf4)\n",
    "\n",
    "rmse_test_dt1 = evaluator.evaluate(test_predictions_dt1)\n",
    "rmse_train_dt1 = evaluator.evaluate(train_predictions_dt1)\n",
    "print(\"Decision Tree MaxDepth=10 - RMSE on test data = %g\" % rmse_test_dt1)\n",
    "print(\"Decision Tree MaxDepth=10 - RMSE on train data = %g\" % rmse_train_dt1)\n",
    "\n",
    "rmse_test_dt2 = evaluator.evaluate(test_predictions_dt2)\n",
    "rmse_train_dt2 = evaluator.evaluate(train_predictions_dt2)\n",
    "print(\"Decision Tree MaxDepth=5 - RMSE on test data = %g\" % rmse_test_dt2)\n",
    "print(\"Decision Tree MaxDepth=5 - RMSE on train data = %g\" % rmse_train_dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen, the Random Forest model with max depth = 10 and 20 decision trees\n",
    " produces the best result for the <b>Test Set</b> with an RMSE of 11.3508, and a\n",
    "  Random Forest model with max depth = 10 and 20 decision trees produces the best result on the\n",
    "  <b>Train Set</b> with an RMSE of 11.1468. <br>\n",
    "  Overall it doesn't seem like one model is boldly preferable than another, and a Decision Tree of max depth = 10 seems to produce relatively good results. <br><br>\n",
    "  It is worth mentioning, that we suffer from imbalanced data, since high Precipitation comes from places of high altitudes and high summer temperatures we have significantly less\n",
    "  observations of high Precipitation, so our model may have trouble predicting high amounts of Precipitation accurately.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}